---
title: "Week 12 Working with words"
author: "Sally Heo"
date: "4/18/2022"
output:
  prettydoc::html_pretty:
    theme: tactile
---

```{r setup, include=FALSE, message=FALSE, echo=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries needed to code ##
```{r,message=FALSE, echo=FALSE, warning=FALSE}
library(here) #Load here
library(tidyverse) #Load tidyverse
library(wordcloud2) #Load wordcloud2
library(janeaustenr) #Load janeaustenr
library(tidytext) #Load tidytext
```
## Intro to {stringr} ##
* String has quotations
* String and a character are the same thing
```{r}
words<-"This is a string" #Example of a string
words
words_vector<-c("Apples", "Bananas", "Oragnes")#Several strings in a vector
words_vector
```
## Manipulation ##
*Pasting words together
* Useful when you have two columns of treatments and you want to combine them into one (e.g., high temp, low temp, and high pH, low)
```{r}
paste("High temp", "Low pH")
paste("High temp", "Low pH", sep = "-") #Add a dash in between the words
paste0("High temp", "Low pH")#Remove the space in between the words
shapes <- c("Square", "Circle", "Triangle")#Working with vectors
paste("My favorite shape is a", shapes)
two_cities <- c("best", "worst")
paste("It was the", two_cities, "of times.")# Useful when making labels for your plots
```

## Manipulation: individual characters ##
```{r}
shapes #vector of shapes
str_length(shapes) #how may letters are in each word
```

## Extracting specific characters ##
* Useful to extract specific bases in a sequence
```{r}
seq_data<-c("ATCCCGTC")
str_sub(seq_data, start = 2, end = 4) #extract the 2nd to 4th AA
str_sub(seq_data, start = 3, end = 3) <- "A" #Modify strings and add an A in the 3rd position
seq_data
str_dup(seq_data, times = c(2, 3)) #duplicate patterns in your strings.times is the number of times to duplicate
```
## Whitespace ##
```{r}
badtreatments<-c("High", "High", "High", "Low", "Low")
badtreatments
str_trim(badtreatments)#this removes both, remove white space
str_trim(badtreatments, side = "left") #this removes left
str_pad(badtreatments, 5, side = "right")#str_pad adds white space to either side, adds a white space to the right side after the 5th character
str_pad(badtreatments, 5, side = "right", pad = "1")#Add a character instead of white space, add a 1 to the right side of the 5th character
```
## Locale sensitive ##
```{r}
x<-"I love R!" 
str_to_upper(x)#Make everything upper case
str_to_lower(x)#Make it lower case
str_to_title(x)#Make it title case(cap first letter of each word)
```
## Pattern matching ##
```{r}
data<-c("AAA", "TATA", "CTAG", "GCTT")
str_view(data, pattern = "A")#find all the strings with an A
str_detect(data, pattern = "A") #detect a specific pattern
str_detect(data, pattern = "AT")
str_locate(data, pattern = "AT") #locate a pattern
```

## regex: regular expressions ##
## Metacharacters ##
```{r}
vals<-c("a.b", "b.c","c.d")#Following set of strings
str_replace(vals, "\\.", " ")#string, pattern, replace all the "." with a space
vals<-c("a.b.c", "b.c.d", "c.d.e")
str_replace(vals, "\\.", " ")#Only replaces the first instance
str_replace_all(vals, "\\.", " ")
```
## Sequences ##
```{r}
val2<-c("test 123", "test 456", "test") #Subset the vector to only keep strings with digits
str_subset(val2, "\\d")
```
## Character class ##
```{r}
str_count(val2, "[aeiou]") #count the number of lowercase vowels in each string
str_count(val2, "[0-9]")# count any digit
```
## Example: find the phone numbers ##
```{r}
strings<-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739")
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
str_detect(strings, phone) #Which strings contain phone numbers?
test<-str_subset(strings, phone) #Subset only the strings with phone numbers
test 
```
## Think, pair, share ##
* Replace all the "." with "_" and extract only the numbers (leaving the letters behind). 
* Remove any extra white space
* Can use a sequence of pipes
```{r}
test %>% 
  str_replace_all(pattern = "\\.", replacement = "_") %>% #replace periods with -
  str_replace_all(pattern = "[a-zA-Z]|\\:", replacement = "") %>% #remove all the things we don't want 
  str_trim() #trim the white space
```
## tidytext ##
* Package for text mining and making text tidy
* Helpful for social sciences or anyone that uses survey data
```{r}
head(austen_books()) #Explore it, To get all of the text from all of Jane Austen's books
tail(austen_books())
original_books <- austen_books() %>% # get all of Jane Austen's books
  group_by(book) %>% 
  mutate(line = row_number(),# find every line
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", #count the chapters (starts with the word chapter followed by a digit or roman numeral)
                                                 ignore_case = TRUE)))) %>% #ignore lower or uppercase 
  ungroup() #ungroup it so we have a dataframe again
head(original_books) #don't try to view the entire thing... its >73000 lines...

tidy_books <- original_books %>% 
  unnest_tokens(output = word, input = text)# add a column named word with the input as the text column
head(tidy_books) #there are now >725,000 rows. Don't view the whole thing
head(get_stopwords())#example of all the stopwords
cleaned_books <- tidy_books %>% 
  anti_join(get_stopwords()) #dataframe without the stopwords, ##Joining, by = "word"
head(cleaned_books)
cleaned_books %>% 
  count(word, sort = TRUE)#count the most common words across all her books
```
## Sentiment analysis ##
```{r}
sent_word_counts <- tidy_books %>% 
  inner_join(get_sentiments()) %>% #only keeps pos or negative words
  count(word, sentiment, sort = TRUE)#count them
```
## GGplot - Visualize counts of positive and negative words in the books
```{r}
sent_word_counts %>% 
  filter(n > 150) %>% #take only if there are over 150 instances of it
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% #add a column where if the word is negative make the count negative
  mutate(word = reorder(word,n)) %>% #sort it so it goes from largest to smallest
  ggplot(aes(word, n, fill = sentiment))+
  geom_col()+
  coord_flip()+
  labs(y = "Contribution to sentiment")
```
## Making a wordcloud ##
* Making an interactive word cloud
```{r}
words<-cleaned_books %>% 
  count(word) %>% #count all the words
  arrange(desc(n)) %>% #sort the words
  slice(1:100)#take the top 100
wordcloud2(words, shape = 'triangle', size=0.3)# make a wordcloud out of the top 100 words
```



